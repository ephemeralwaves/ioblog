{
  
    
        "post0": {
            "title": "Poison Ivy Classifier",
            "content": "note: when in doubt about any plant, do not touch! This program is experimental with a 25% error rate! It is trained to classify poison ivy look alikes like the virginia creeper, boston ivy, box elder, and kudzu. . Upload a picture of a plant that you think looks like or is poison ivy! . from fastai.vision.all import * from fastai.vision.widgets import * path = Path() learn_inf = load_learner(path/&#39;export.pkl&#39;, cpu=True) btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() def on_data_change(change): lbl_pred.value = &#39;&#39; img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_upload.observe(on_data_change, names=[&#39;data&#39;]) display(VBox([widgets.Label(&#39;Select your Plant!&#39;), btn_upload, out_pl, lbl_pred])) . Further Research . The model is limited with the data I trained it with, so for instance if I show it a picture of daisies it will try to fit it into one of the categories it knows. It was also suprising that it tends to mix up Virginia creeper up with poison ivy since by human standards it&#39;s easy to tell them apart by counting the number of leaves. Of course it has no notion of what a leaf is so I&#39;m guessing that&#39;s why. . If I&#39;m to iterate over this project again I&#39;d probably try to incorporate more transfer learning by finding a model that has been trained to recognize a plant data set such as (https://www.kaggle.com/apollonius/usda-plant-database) or something like this. .",
            "url": "https://ephemeralwaves.github.io/ioblog/2021/07/18/IvyDetection_webapp.html",
            "relUrl": "/2021/07/18/IvyDetection_webapp.html",
            "date": " • Jul 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Creating and Pushing a Classifier Into Production with FastAI",
            "content": "note: when running this in colab, remember to set runtime to use a gpu! . From Model to Production . In line with my work creating an ecosystem building game which helps kids &amp; adults of all ages get familiar with local flora and fauna, I&#39;m going to build a poison ivy classifier. . Starting My Project . I took the advice of the course and kept in mind that: &quot;[...] the the most important consideration is data availability. The goal is not to find the &quot;perfect&quot; dataset or project, but just to get started and iterate from there.&quot; . The Drivetrain Approach . There are many accurate models that are of no use to anyone, and many inaccurate models that are highly useful. To ensure that your modeling work is useful in practice, you need to consider how your work will be used. In 2012 Jeremy, along with Margit Zwemer and Mike Loukides, introduced a method called the Drivetrain Approach for thinking about this issue. . . Objective: Identify images of poison ivy when uploading a picture of a plant found in the wild. | Levers(what actions you can take): What sorts of data make it into the set, ie using poison ivy and look alikes so the model can distinguish | Data: Images of known poison ivy plants and look alikes. Datasets must include leaves, flowers, roots. | . Only after these first three steps do we begin thinking about building the predictive models. Our objective and available levers, what data we already have and what additional data we will need to collect, determine the models we can build. The models will take both the levers and any uncontrollable variables as their inputs; the outputs from the models can be combined to predict the final state for our objective. . Model: Shows probability it is poison ivy or not and always recommends you stay away if you&#39;re not sure ;) | . Gathering Data . For many types of projects, you may be able to find all the data you need online. The project we&#39;ll be completing in this chapter is a poison ivy detector. It will discriminate between these types of plants: poison ivy, japenese kudzu,aromatic sumac, boxelder, Boston ivy, and Virginia creeper. . I will be using Bing Image Search/Azure Cognitive Services. . Search . To download images with Bing Image Search, I signed up at Microsoft. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;xxx&#39;) . Or, if you&#39;re comfortable at the command line, you can set it in your terminal with: . export AZURE_SEARCH_KEY=your_key_here . and then restart Jupyter Notebook, and use the above line without editing it. . Once you&#39;ve set key, you can use search_images_bing. This function is provided by the small utils class included with the notebooks online. If you&#39;re not sure where a function is defined, you can just type it in your notebook to find out: . search_images_bing . &lt;function fastbook.search_images_bing&gt; . results = search_images_bing(key, &#39;Poison Ivy leaves plants&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . print(ims[3]) . https://bygl.osu.edu/sites/default/files/field/image/poison_ivy_as.jpg . We&#39;ve successfully downloaded the URLs of 150 poison ivy plants (or, at least, images that Bing Image Search finds for that search term). Let&#39;s look at one: . os.mkdir(&quot;images/&quot;) dest = &#39;images/poisonivy.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . This seems to have worked nicely, so let&#39;s use fastai&#39;s download_images to download all the URLs for each of our search terms. We&#39;ll put each in a separate folder: . plant_types = &#39;poison ivy&#39;, &#39;virginia creeper&#39;, &#39;kudzu&#39;, &#39;aromatic sumac&#39;, &#39;boxelder&#39;, &#39;boston ivy&#39; path = Path(&#39;plants&#39;) . try: path.mkdir() except FileExistsError: pass for o in plant_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} leaves leaf&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . Our folder has image files, as we&#39;d expect: . fns = get_image_files(path) fns . (#785) [Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000000.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000001.JPG&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000006.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000003.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000005.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000002.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000018.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000014.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000009.jpg&#39;),Path(&#39;/content/gdrive/My Drive/datasets/plants/boxelder/00000016.jpg&#39;)...] . Often when we download files from the internet, there are a few that are corrupt. Let&#39;s check: . failed = verify_images(fns) failed . (#0) [] . To remove all the failed images, you can use unlink on each of them. Note that, like most fastai functions that return a collection, verify_images returns an object of type L, which includes the map method. This calls the passed function on each element of the collection: . failed.map(Path.unlink); . Now that we have downloaded some data, we need to assemble it in a format suitable for model training. In fastai, that means creating an object called DataLoaders. . Uploading a dataset . For this particular problem, identifying poison ivy, pulling from bing image search lead to a lot of mistakes in the data since poison ivy is easily mistaken for the other plants we are training the model to recognize. So I ran the code above locally and curated it before uploading since using Colab was a pain in terms of curation. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) . Mounted at /content/gdrive . plant_types = &#39;poison ivy&#39;, &#39;virginia creeper&#39;, &#39;kudzu&#39;, &#39;aromatic sumac&#39;, &#39;boxelder&#39;, &#39;boston ivy&#39; path = Path(&#39;/content/gdrive/My Drive/datasets/plants&#39;) . From Data to DataLoaders . jargon:DataLoaders: A fastai class that stores multiple DataLoader objects you pass to it, normally a train and a valid, although it&#39;s possible to have as many as you like. The first two are made available as properties. . To turn our downloaded data into a DataLoaders object we need to tell fastai at least four things: . What kinds of data we are working with | How to get the list of items | How to label these items | How to create the validation set | . So far we have seen a number of factory methods for particular combinations of these things, which are convenient when you have an application and data structure that happen to fit into those predefined methods. For when you don&#39;t, fastai has an extremely flexible system called the data block API. With this API you can fully customize every stage of the creation of your DataLoaders. Here is what we need to create a DataLoaders for the dataset that we just downloaded: . plants = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . Let&#39;s look at each of these arguments in turn. First we provide a tuple where we specify what types we want for the independent and dependent variables: . blocks=(ImageBlock, CategoryBlock) . The independent variable is the thing we are using to make predictions from, and the dependent variable is our target. In this case, our independent variables are images, and our dependent variables are the categories (type of plant) for each image. . For this DataLoaders our underlying items will be file paths. We have to tell fastai how to get a list of those files. The get_image_files function takes a path, and returns a list of all of the images in that path (recursively, by default): . get_items=get_image_files . Often, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV file in which each filename is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach that allows you to use one of its predefined classes for this, or to write your own. In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed (computers don&#39;t really know how to create random numbers at all, but simply create lists of numbers that look random; if you provide the same starting point for that list each time—called the seed—then you will get the exact same list each time): . splitter=RandomSplitter(valid_pct=0.2, seed=42) . The independent variable is often referred to as x and the dependent variable is often referred to as y. Here, we are telling fastai what function to call to create the labels in our dataset: . get_y=parent_label . parent_label is a function provided by fastai that simply gets the name of the folder a file is in. Because we put each of our bear images into folders based on the type of bear, this is going to give us the labels that we need. . Our images are all different sizes, and this is a problem for deep learning: we don&#39;t feed the model one image at a time but several of them (what we call a mini-batch). To group them in a big array (usually called a tensor) that is going to go through our model, they all need to be of the same size. So, we need to add a transform which will resize these images to the same size. Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we use the Resize transform here: . item_tfms=Resize(128) . This command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found: . dls = plants.dataloaders(path) . A DataLoaders includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. We&#39;ll be learning a lot more about this class in the next chapter. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader: . dls.valid.show_batch(max_n=4, nrows=1) . By default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them: . plants = plants.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = plants.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . plants = plants.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = plants.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . All of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use. . Instead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways. . In fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn&#39;t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image. . Here&#39;s another example where we replace Resize with RandomResizedCrop, which is the transform that provides the behavior we just described. The most important parameter to pass in is min_scale, which determines how much of the image to select at minimum each time: . plants = plants.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = plants.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . We used unique=True to have the same image repeated with different versions of this RandomResizedCrop transform. This is a specific example of a more general technique, called data augmentation. . Data Augmentation . Data augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. For natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the aug_transforms function. Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms parameter (note that we&#39;re not using RandomResizedCrop in this example, so you can see the differences more clearly; we&#39;re also using double the amount of augmentation compared to the default, for the same reason): . plants = plants.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = plants.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Now that we have assembled our data in a format fit for model training, let&#39;s actually train an image classifier using it. . Training Your Model, and Using It to Clean Your Data . Time to use the same lines of code as in &lt;&gt; to train our plant classifier.&lt;/p&gt; We don&#39;t have a lot of data for our problem (150 pictures of each sort of plant at most), so to train our model, we&#39;ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; plants = plants.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = plants.dataloaders(path) . We can now create our Learner and fine-tune it in the usual way: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(8) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 2.662767 | 1.677851 | 0.592357 | 00:25 | . epoch train_loss valid_loss error_rate time . 0 | 1.725976 | 1.251074 | 0.452229 | 00:25 | . 1 | 1.568583 | 1.038941 | 0.375796 | 00:26 | . 2 | 1.357989 | 0.989407 | 0.337580 | 00:25 | . 3 | 1.144731 | 0.901760 | 0.267516 | 00:25 | . 4 | 0.946494 | 0.873243 | 0.248408 | 00:25 | . 5 | 0.801797 | 0.864572 | 0.261146 | 00:25 | . 6 | 0.674549 | 0.856466 | 0.254777 | 00:25 | . 7 | 0.588035 | 0.846164 | 0.248408 | 00:25 | . Now let&#39;s see whether the mistakes the model is making are mainly thinking that virginia creepers are poison ivy or that poison ivy are boston ivy, or something else. To visualize this, we can create a confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . With the color-coding, the goal is to have white everywhere except the diagonal, where we want dark blue. . It&#39;s helpful to see where exactly our errors are occurring, to see whether they&#39;re due to a dataset problem (e.g., images that aren&#39;t plants at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn&#39;t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss. . The loss is a number that is higher if the model is incorrect (especially if it&#39;s also confident of its incorrect answer), or if it&#39;s correct, but not confident of its correct answer. For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction: . interp.plot_top_losses(5, nrows=5) . The intuitive approach to doing data cleaning is to do it before you train a model. But as you&#39;ve seen in this case, a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning. . fastai includes a handy GUI for data cleaning called ImageClassifierCleaner that allows you to choose a category and the training versus validation set and view the highest-loss images (in order), along with menus to allow images to be selected for removal or relabeling: . cleaner = ImageClassifierCleaner(learn) cleaner . for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . Now that we have trained our model, let&#39;s see how we can deploy it to be used in practice. . Turning Your Model into an Online Application . We are now going to look at what it takes to turn this model into a working online application. We will just go as far as creating a basic working prototype; we do not have the scope in this book to teach you all the details of web application development generally. . Using the Model for Inference . Once you&#39;ve got a model you&#39;re happy with, you need to save it, so that you can then copy it over to a server where you&#39;ll use it in production. Remember that a model consists of two parts: the architecture and the trained parameters. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the export method. . This method even saves the definition of how to create your DataLoaders. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set DataLoader for inference by default, so your data augmentation will not be applied, which is generally what you want. . When you call export, fastai will save a file called &quot;export.pkl&quot;: . learn.export() . Let&#39;s check that the file exists, by using the ls method that fastai adds to Python&#39;s Path class: . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . You&#39;ll need this file wherever you deploy your app to. For now, let&#39;s try to create a simple app within our notebook. . When we use a model for getting predictions, instead of training, we call it inference. To create our inference learner from the exported file, we use load_learner (in this case, this isn&#39;t really necessary, since we already have a working Learner in our notebook; we&#39;re just doing it here so you can see the whole process end-to-end): . learn_inf = load_learner(path/&#39;export.pkl&#39;) . When we&#39;re doing inference, we&#39;re generally just getting predictions for one image at a time. To do this, pass a filename to predict: . learn_inf.predict(&#39;images/poisonivy.jpg&#39;) . This has returned three things: the predicted category in the same format you originally provided (in this case that&#39;s a string), the index of the predicted category, and the probabilities of each category. The last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories. At inference time, you can access the DataLoaders as an attribute of the Learner: . learn_inf.dls.vocab . (#6) [&#39;aromatic sumac&#39;,&#39;boston ivy&#39;,&#39;boxelder&#39;,&#39;kudzu&#39;,&#39;poison ivy&#39;,&#39;virginia Creeper&#39;] . Creating a Notebook App from the Model . To use our model in an application, we can simply treat the predict method as a regular function. Therefore, creating an app from the model can be done using any of the myriad of frameworks and techniques available to application developers. . However, most data scientists are not familiar with the world of web application development. So let&#39;s try using something that you do, at this point, know: it turns out that we can create a complete working web application using nothing but Jupyter notebooks! The two things we need to make this happen are: . IPython widgets (ipywidgets) | Voilà | . IPython widgets are GUI components that bring together JavaScript and Python functionality in a web browser, and can be created and used within a Jupyter notebook. For instance, the image cleaner that we saw earlier in this chapter is entirely written with IPython widgets. However, we don&#39;t want to require users of our application to run Jupyter themselves. . That is why Voilà exists. It is a system for making applications consisting of IPython widgets available to end users, without them having to use Jupyter at all. Voilà is taking advantage of the fact that a notebook already is a kind of web application, just a rather complex one that depends on another web application: Jupyter itself. Essentially, it helps us automatically convert the complex web application we&#39;ve already implicitly made (the notebook) into a simpler, easier-to-deploy web application, which functions like a normal web application rather than like a notebook. . But we still have the advantage of developing in a notebook, so with ipywidgets, we can build up our GUI step by step. We will use this approach to create a simple image classifier. First, we need a file upload widget: . btn_upload = widgets.FileUpload() btn_upload . . Now we can grab the image: . img = PILImage.create(btn_upload.data[-1]) . We can use an Output widget to display it: . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . and use a Label to display them: . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Prediction: poison ivy; Probability: .9997 . We&#39;ll need a button to do the classification. It looks exactly like the upload button: . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . We&#39;ll also need a click event handler; that is, a function that will be called when it&#39;s pressed. We can just copy over the lines of code from above: . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . You can test the button now by pressing it, and you should see the image and predictions update automatically! . We can now put them all in a vertical box (VBox) to complete our GUI: . VBox([widgets.Label(&#39;Select your plant!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Further Research . The model is limited with the data I trained it with, so for instance if I show it a picture of daisies it will try to fit it into one of the categories it knows. It was also suprising that it tends to mix up Virginia creeper up with poison ivy since by human standards it&#39;s easy to tell them apart by counting the number of leaves. Of course it has no notion of what a leaf is so I&#39;m guessing that&#39;s why. . If I&#39;m to iterate over this project again I&#39;d probably try to incorporate more transfer learning by finding a model that has been trained to recognize a plant data set such as (https://www.kaggle.com/apollonius/usda-plant-database) or something like this. . &lt;/div&gt; .",
            "url": "https://ephemeralwaves.github.io/ioblog/2021/07/18/IvyDetection_production.html",
            "relUrl": "/2021/07/18/IvyDetection_production.html",
            "date": " • Jul 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ephemeralwaves.github.io/ioblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ephemeralwaves.github.io/ioblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ephemeralwaves.github.io/ioblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ephemeralwaves.github.io/ioblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}